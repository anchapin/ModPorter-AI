#!/usr/bin/env python3
"""
Simple Test Generator for ModPorter-AI
Focuses on generating basic tests to improve coverage quickly
"""

import ast
import json
from pathlib import Path
from typing import Dict, List, Any

def analyze_file(file_path: Path) -> Dict[str, Any]:
    """Analyze a Python file and extract test-relevant information"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            source_code = f.read()
        
        tree = ast.parse(source_code)
        
        analysis = {
            "file_path": str(file_path),
            "source_code": source_code,
            "classes": [],
            "functions": [],
            "test_candidates": []
        }
        
        # Extract classes and functions
        for node in tree.body:
            if isinstance(node, ast.ClassDef):
                class_info = {
                    "name": node.name,
                    "methods": []
                }
                
                for item in node.body:
                    if isinstance(item, (ast.FunctionDef, ast.AsyncFunctionDef)):
                        if not item.name.startswith("_"):
                            class_info["methods"].append(item.name)
                            analysis["test_candidates"].append({
                                "type": "method",
                                "class": node.name,
                                "name": item.name,
                                "is_async": isinstance(item, ast.AsyncFunctionDef)
                            })
                
                analysis["classes"].append(class_info)
            
            elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                if not node.name.startswith("_"):
                    func_info = {
                        "name": node.name,
                        "is_async": isinstance(node, ast.AsyncFunctionDef)
                    }
                    analysis["functions"].append(func_info)
                    
                    analysis["test_candidates"].append({
                        "type": "function",
                        "name": node.name,
                        "is_async": isinstance(node, ast.AsyncFunctionDef)
                    })
        
        return analysis
        
    except Exception as e:
        return {"error": str(e), "file_path": str(file_path)}

def generate_basic_tests(analysis: Dict[str, Any]) -> str:
    """Generate basic test cases"""
    if "error" in analysis:
        return f"# Error analyzing file: {analysis['error']}\n"
    
    content = []
    
    # Add header
    content.append('"""')
    content.append(f'Auto-generated tests for {Path(analysis["file_path"]).name}')
    content.append('Generated by simple_test_generator.py')
    content.append('"""')
    content.append('')
    
    # Add imports
    content.append('import pytest')
    content.append('from unittest.mock import Mock, patch, AsyncMock')
    content.append('import sys')
    content.append('import os')
    content.append('')
    
    # Add project path
    content.append('sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))')
    content.append('')
    
    # Generate tests for each candidate
    for candidate in analysis["test_candidates"]:
        test_prefix = "test_async_" if candidate.get("is_async", False) else "test_"
        
        if candidate["type"] == "method":
            full_name = f"{candidate['class']}_{candidate['name']}"
        else:
            full_name = candidate["name"]
        
        # Basic test
        content.append(f'def {test_prefix}{full_name}_basic():')
        content.append(f'    """Basic test for {full_name}"""')
        content.append('    # Setup test data based on function patterns')
        content.append('    try:')
        content.append('        # Test with typical inputs')
        content.append('        test_inputs = [')
        content.append('            "test_string",  # String input')
        content.append('            42,            # Integer input')
        content.append('            3.14,          # Float input')
        content.append('            True,          # Boolean input')
        content.append('        ]')
        content.append('        ')
        content.append('        # Try calling with different input combinations')
        content.append('        for test_input in test_inputs[:1]:  # Test first parameter type')
        content.append('            try:')
        content.append(f'                result = {candidate["module"].replace("/", ".")}.{full_name}(test_input)')
        content.append('                assert result is not None or f"{candidate["module"].replace("/", ".")}.{full_name}" in ["__repr__", "__str__"]')
        content.append('            except (ValueError, TypeError, AttributeError):')
        content.append('                pass  # Expected for incompatible types')
        content.append('                ')
        content.append('        # Test with no parameters if function supports it')
        content.append('        try:')
        content.append(f'            result = {candidate["module"].replace("/", ".")}.{full_name}()')
        content.append('            assert True  # Function completed without error')
        content.append('        except:')
        content.append('            pass  # Function may require parameters')
        content.append('        ')
        content.append('    except ImportError:')
        content.append('        pytest.skip(f"Could not import {candidate["module"].replace("/", ".")}")')
        content.append('    except Exception as e:')
        content.append('        assert isinstance(e, (ValueError, TypeError, AttributeError))')
        content.append('')

        # Edge case test
        content.append(f'def {test_prefix}{full_name}_edge_cases():')
        content.append(f'    """Edge case tests for {full_name}"""')
        content.append('    # Test edge cases, error conditions')
        content.append('    edge_cases = [')
        content.append('        None,           # None values')
        content.append('        "",             # Empty strings')
        content.append('        0,              # Zero values')
        content.append('        -1,             # Negative values')
        content.append('        [],             # Empty collections')
        content.append('        {},             # Empty dicts')
        content.append('    ]')
        content.append('    ')
        content.append('    for edge_case in edge_cases:')
        content.append('        try:')
        content.append(f'            result = {candidate["module"].replace("/", ".")}.{full_name}(edge_case)')
        content.append('            # Should handle edge cases gracefully or raise expected exceptions')
        content.append('            assert True')
        content.append('        except (ValueError, TypeError, AttributeError, IndexError, KeyError):')
        content.append('            pass  # Expected exceptions for edge cases')
        content.append('    ')
        content.append('')

        # Error handling test
        content.append(f'def {test_prefix}{full_name}_error_handling():')
        content.append(f'    """Error handling tests for {full_name}"""')
        content.append('    # Test error conditions and exceptions')
        content.append('    invalid_inputs = [')
        content.append('        object(),       # Invalid object types')
        content.append('        lambda: None,  # Functions (usually invalid for primitive ops)')
        content.append('        float("inf"),   # Infinity')
        content.append('        float("nan"),   # Not a number')
        content.append('    ]')
        content.append('    ')
        content.append('    for invalid_input in invalid_inputs:')
        content.append('        try:')
        content.append(f'            result = {candidate["module"].replace("/", ".")}.{full_name}(invalid_input)')
        content.append('            # If no exception, function should handle gracefully')
        content.append('            assert True')
        content.append('        except (ValueError, TypeError, OverflowError, AttributeError):')
        content.append('            pass  # Expected exceptions for invalid inputs')
        content.append('        except Exception as e:')
        content.append('            # Unexpected exceptions should be investigated')
        content.append('            if "RecursionError" not in str(e) and "MemoryError" not in str(e):')
        content.append('                raise AssertionError(f"Unexpected exception: {{e}}")')
        content.append('    ')
        content.append('')
    
    return "\n".join(content)

def main():
    """Main function"""
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python simple_test_generator.py <target_file_or_directory>")
        sys.exit(1)
    
    target = sys.argv[1]
    project_root = Path.cwd()
    target_path = project_root / target
    
    if target_path.is_file():
        # Generate tests for single file
        print(f"Generating tests for {target_path}")
        
        analysis = analyze_file(target_path)
        test_content = generate_basic_tests(analysis)
        
        # Determine test file path
        test_dir = project_root / "tests"
        test_file_path = test_dir / f"test_{target_path.stem}.py"
        
        # Write test file
        test_dir.mkdir(exist_ok=True)
        with open(test_file_path, 'w', encoding='utf-8') as f:
            f.write(test_content)
        
        print(f"Generated test file: {test_file_path}")
        
    elif target_path.is_dir():
        # Generate tests for all Python files in directory
        for py_file in target_path.rglob("*.py"):
            if "test_" not in py_file.name:
                print(f"Generating tests for {py_file}")
                
                analysis = analyze_file(py_file)
                test_content = generate_basic_tests(analysis)
                
                # Determine test file path (relative to target)
                rel_path = py_file.relative_to(target_path)
                test_rel_path = Path("tests") / f"test_{rel_path.stem}.py"
                test_file_path = project_root / test_rel_path
                
                # Create directories if needed
                test_file_path.parent.mkdir(parents=True, exist_ok=True)
                
                # Write test file
                with open(test_file_path, 'w', encoding='utf-8') as f:
                    f.write(test_content)
                
                print(f"Generated test file: {test_file_path}")
    else:
        print(f"Target not found: {target_path}")
        sys.exit(1)
    
    print("\nTest generation completed!")
    print("Note: Generated tests are placeholders - implement actual test logic")

if __name__ == "__main__":
    main()
