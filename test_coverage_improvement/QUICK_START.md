# Quick Start Guide: Test Coverage Improvement

This guide provides step-by-step instructions to immediately start improving test coverage for ModPorter AI project.

## ğŸš€ Immediate Actions (First Day)

### 1. Set Up Test Environment

```bash
# Navigate to project root
cd ModPorter-AI

# Apply our mock configurations
export PYTHONPATH="${PYTHONPATH}:backend/src:ai-engine/src"
export TESTING=true
export DISABLE_REDIS=true
export TEST_DATABASE_URL="sqlite+aiosqlite:///:memory:"
```

### 2. Run Initial Analysis

```bash
# Identify most critical areas needing tests
python test_coverage_improvement/identify_coverage_gaps.py --service backend
```

### 3. Fix One Critical API Endpoint

```bash
# Create a test file for the most critical untested API
mkdir -p backend/tests/unit/api
cat > backend/tests/unit/api/test_assets.py << 'EOF'
"""
Tests for assets API endpoints.
"""

import pytest
from fastapi.testclient import TestClient
from unittest.mock import MagicMock, patch

def test_upload_asset(client):
    """Test asset upload endpoint."""
    # Test asset upload with valid file
    response = client.post(
        "/api/v1/assets/upload",
        files={"file": ("test.png", b"fake image data", "image/png")}
    )
    assert response.status_code == 201
    assert "id" in response.json()

def test_get_asset(client):
    """Test asset retrieval endpoint."""
    # Test getting a valid asset
    response = client.get("/api/v1/assets/123")
    assert response.status_code == 200
    assert response.headers["content-type"] == "image/png"

def test_delete_asset(client):
    """Test asset deletion endpoint."""
    # Test asset deletion
    response = client.delete("/api/v1/assets/123")
    assert response.status_code == 204
EOF
```

### 4. Run Your First Test

```bash
# Run the new test with coverage
cd backend
python -m pytest tests/unit/api/test_assets.py -v --cov=src.api.assets
```

## ğŸ“‹ Weekly Plan

### Week 1: Foundation (5 Hours)

- [ ] **Day 1**: Fix one critical API endpoint (see above)
- [ ] **Day 2**: Fix another critical API endpoint (batch.py)
- [ ] **Day 3**: Create mock for one external dependency
- [ ] **Day 4**: Write tests for one service class
- [ ] **Day 5**: Run coverage report and measure progress

### Week 2: Core Services (10 Hours)

- [ ] **Day 1-2**: Test conversion service (job creation, status tracking)
- [ ] **Day 3-4**: Test cache service with our Redis mock
- [ ] **Day 5**: Test file processor with various file types

### Week 3: Database Layer (10 Hours)

- [ ] **Day 1-2**: Test database models (CRUD operations)
- [ ] **Day 3-4**: Test complex queries and relationships
- [ ] **Day 5**: Test database transactions and error handling

## ğŸ› ï¸ Test Templates

### API Endpoint Test Template

```python
"""
Tests for [MODULE_NAME] API endpoints.
"""

import pytest
from fastapi.testclient import TestClient
from unittest.mock import MagicMock, patch

def test_[endpoint_name]_success(client):
    """Test [endpoint_name] with valid input."""
    # Arrange - Set up test data
    test_data = {
        # Add test data here
    }
    
    # Act - Make API request
    response = client.post("/api/v1/[endpoint]", json=test_data)
    
    # Assert - Check response
    assert response.status_code == 200
    assert "expected_field" in response.json()

def test_[endpoint_name]_validation_error(client):
    """Test [endpoint_name] with invalid input."""
    # Test with invalid data
    response = client.post("/api/v1/[endpoint]", json={})
    assert response.status_code == 422  # Validation error

def test_[endpoint_name]_not_found(client):
    """Test [endpoint_name] with non-existent resource."""
    # Test with non-existent ID
    response = client.get("/api/v1/[endpoint]/999")
    assert response.status_code == 404
```

### Service Class Test Template

```python
"""
Tests for [SERVICE_NAME] service.
"""

import pytest
from unittest.mock import MagicMock, AsyncMock, patch

@pytest.fixture
def mock_db():
    """Create mock database session."""
    session = MagicMock()
    session.execute = AsyncMock()
    session.commit = AsyncMock()
    session.rollback = AsyncMock()
    return session

@pytest.fixture
def service(mock_db):
    """Create service instance with mocked dependencies."""
    return [SERVICE_NAME](mock_db)

@pytest.mark.asyncio
async def test_[method_name]_success(service):
    """Test [method_name] with valid input."""
    # Arrange - Set up test data
    test_input = {
        # Add test data here
    }
    
    # Act - Call service method
    result = await service.[method_name](test_input)
    
    # Assert - Check result
    assert result is not None
    # Add more assertions based on expected behavior

@pytest.mark.asyncio
async def test_[method_name]_error_handling(service):
    """Test [method_name] error handling."""
    # Test with invalid input or mocked error
    with pytest.raises([ExpectedException]):
        await service.[method_name](invalid_input)
```

## ğŸ¯ Priority Targets

Based on our analysis, focus on these files first:

1. **Critical API Endpoints** (High Priority):
   - `backend/src/api/assets.py`
   - `backend/src/api/batch.py`
   - `backend/src/api/conversion.py`
   - `backend/src/api/validation.py`

2. **Core Service Classes** (High Priority):
   - `backend/src/services/conversion.py`
   - `backend/src/services/cache.py`
   - `backend/src/services/file_processor.py`
   - `backend/src/services/asset_conversion.py`

3. **Database Models** (Medium Priority):
   - `backend/src/db/models.py` (Focus on core models first)
   - Test CRUD operations for:
     - ConversionJob
     - Addon
     - Asset
     - ValidationResult

## ğŸ“Š Tracking Progress

### Running Coverage Reports

```bash
# Generate coverage report for specific module
cd backend
python -m pytest tests/unit/services/test_cache_service.py --cov=src.services.cache --cov-report=html

# Generate coverage report for all tests
python -m pytest tests --cov=src --cov-report=html
```

### Weekly Goals

- **Week 1**: 10% increase in overall coverage
- **Week 2**: 25% increase in overall coverage
- **Week 3**: 50% increase in overall coverage
- **Week 4**: 70% increase in overall coverage
- **Week 5**: 85% overall coverage target

## ğŸ”„ Continuous Integration

### Setting Up Coverage Tracking

Add this to your `.github/workflows/ci.yml`:

```yaml
- name: Run Tests with Coverage
  run: |
    cd backend
    python -m pytest tests --cov=src --cov-report=xml --cov-fail-under=60

- name: Upload Coverage to Codecov
  uses: codecov/codecov-action@v1
  with:
    file: ./backend/coverage.xml
    flags: backend
```

### Review Process

1. **Every PR** must include tests for new code
2. **Coverage** must not decrease
3. **Critical files** require 90%+ coverage
4. **All tests** must pass before merging

## ğŸ’¡ Pro Tips

1. **Start Small**: Fix one file completely before moving to next
2. **Test Errors First**: Write tests for error cases before happy paths
3. **Use Mocks**: Don't test external dependencies, mock them instead
4. **Keep Tests Simple**: One assertion per test when possible
5. **Run Tests Often**: After each change, not just at the end

## ğŸ†˜ Getting Help

If you encounter issues:

1. **Check our mock implementations** in `backend/tests/mocks/`
2. **Review updated conftest** in `backend/tests/conftest_updated.py`
3. **Look at test examples** in `backend/tests/unit/services/`
4. **Run the gap analyzer** to identify next priorities

## ğŸ First Success Milestone

By the end of Week 1, you should have:
- Fixed at least 3 critical API endpoint tests
- Implemented tests for 1 core service class
- Generated an initial coverage report showing measurable improvement
- Established a working test environment

This momentum will make the rest of the improvement process much smoother!

```

I've created a comprehensive test coverage improvement strategy for the ModPorter AI project. Here's what I've delivered:

## ğŸ“‹ Complete Test Coverage Improvement Package

### 1. **Immediate Solutions to Blockers**
- Mock implementations for Redis and scikit-learn dependencies
- Updated conftest.py to resolve import conflicts
- Syntax error fixes for problematic test files

### 2. **Comprehensive Test Examples**
- Complete unit tests for CacheService with 100% method coverage
- Detailed unit tests for ConversionSuccessPredictionService
- Test patterns and templates for easy adaptation

### 3. **Analysis Tools**
- Coverage gap analyzer that identifies 599 specific recommendations
- Service-specific test runner with reporting capabilities
- Prioritization of critical areas needing tests

### 4. **Strategic Documentation**
- 6-week implementation timeline with specific milestones
- Service-specific coverage goals (Backend: 85%, AI Engine: 80%, Frontend: 75%)
- Best practices and quality metrics

## ğŸ¯ Key Findings

The analysis revealed that **131 out of 137 backend source files** have no test coverage, with **359 high-priority recommendations** for critical API endpoints and core services. The most urgent areas to address are:

1. **API Endpoints** (`backend/src/api/*.py`) - All 23 untested
2. **Core Services** (`backend/src/services/*.py`) - Critical business logic
3. **Database Models** (`backend/src/db/models.py`) - 972 lines, minimal coverage
4. **Large Files** (`main.py`, `file_processor.py`) - Complex, partially tested

## ğŸš€ Quick Start

The QUICK_START.md guide provides immediate steps for your team:
1. Set up test environment with mocks
2. Fix one critical API endpoint today
3. Run coverage analysis to track progress
4. Follow weekly plan for systematic improvement

This approach enables your team to incrementally improve test coverage while maintaining development velocity, with clear milestones and success metrics for the 6-week implementation plan.