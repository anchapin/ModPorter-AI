# ai-engine/src/scripts/populate_kb.py
import asyncio
import logging
import os
from typing import List # Required for type hinting List[ScraperDocument]

# Use lightweight chunker for document chunking
from utils.chunker import Chunker

# Adjust import paths based on the actual location relative to ai-engine/src/
# Assuming 'from src.utils...' if running from parent directory of src,
# or 'from ..utils...' if running as a module within src.
# For a script, direct path manipulation or PYTHONPATH adjustment might be needed if imports fail.
# Let's try relative imports assuming it's run as a module for now,
# e.g., python -m src.scripts.populate_kb
try:
    from utils.bedrock_docs_scraper import BedrockDocsScraper, Document as ScraperDocument
    from utils.vector_db_client import VectorDBClient
except ImportError:
    # Fallback for running script directly for development, assuming PYTHONPATH is set or script is in a location
    # where src is discoverable. This might need adjustment based on execution environment.
    import sys
    # This adds 'ai-engine' to sys.path if the script is in 'ai-engine/src/scripts/'
    # and 'ai-engine' is the project root.
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    if project_root not in sys.path:
        sys.path.insert(0, project_root)
    from utils.bedrock_docs_scraper import BedrockDocsScraper, Document as ScraperDocument
    from utils.vector_db_client import VectorDBClient


logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Configuration for chunking (can be made configurable via env vars or CLI args later)
CHUNK_SIZE = 256  # As used in EmbeddingGenerator example
CHUNK_OVERLAP = 32 # As used in EmbeddingGenerator example

async def main():
    logger.info("Starting knowledge base population pipeline...")


    # Use lightweight chunker for document chunking

    scraper = BedrockDocsScraper()
    vector_db_client = VectorDBClient()

    try:
        # 1. Scrape documentation
        logger.info("Scraping Bedrock documentation...")
        scraped_documents: List[ScraperDocument] = await scraper.scrape_documentation()
        if not scraped_documents:
            logger.info("No documents were scraped. Exiting.")
            return
        logger.info(f"Successfully scraped {len(scraped_documents)} initial documents.")

        # 2. Process and index each document
        total_chunks_indexed = 0
        successfully_indexed_docs = 0

        for i, doc in enumerate(scraped_documents):
            logger.info(f"Processing document {i+1}/{len(scraped_documents)}: {doc.source} (Type: {doc.doc_type})")

            # 2a. Chunk document content
            # Assuming doc.content is a string.
            if not doc.content or not isinstance(doc.content, str):
                logger.warning(f"Document {doc.source} has no content or content is not a string. Skipping.")
                continue

            text_chunks = Chunker.chunk_document(
                document=doc.content,
                chunk_size=CHUNK_SIZE,
                overlap=CHUNK_OVERLAP
            )

            if not text_chunks:
                logger.warning(f"Document {doc.source} resulted in no chunks. Skipping.")
                continue

            logger.info(f"Document {doc.source} split into {len(text_chunks)} chunks.")

            # 2b. Index each chunk
            doc_chunks_indexed = 0
            for chunk_index, chunk_content in enumerate(text_chunks):
                # Create a unique source identifier for each chunk, e.g., "url_or_doc_id#chunk_N"
                # This helps in tracing back the chunk to its original document and position.
                chunk_source_id = f"{doc.source}#chunk_{chunk_index}"

                # The content_hash in VectorDBClient.index_document is for the chunk itself.
                # The original doc.content_hash is for the full document before chunking.

                logger.debug(f"Indexing chunk {chunk_index + 1}/{len(text_chunks)} from {doc.source}...")
                success = await vector_db_client.index_document(
                    document_content=chunk_content,
                    document_source=chunk_source_id
                    # content_hash for the chunk will be generated by Document class inside index_document
                    # or by VectorDBClient itself. The `Document` class in vector_db_client.py
                    # generates a hash, but index_document in VectorDBClient calculates its own.
                )
                if success:
                    total_chunks_indexed += 1
                    doc_chunks_indexed += 1
                else:
                    logger.warning(f"Failed to index chunk {chunk_index + 1} from {doc.source}.")

            if doc_chunks_indexed > 0:
                successfully_indexed_docs +=1
            logger.info(f"Indexed {doc_chunks_indexed}/{len(text_chunks)} chunks for document {doc.source}.")

        logger.info(f"Knowledge base population completed.")
        logger.info(f"Successfully processed and attempted indexing for {successfully_indexed_docs}/{len(scraped_documents)} documents.")
        logger.info(f"Total chunks indexed: {total_chunks_indexed}")

    except Exception as e:
        logger.error(f"An error occurred during the knowledge base population pipeline: {e}", exc_info=True)
    finally:
        logger.info("Closing scraper and vector DB client connections...")
        await scraper.close()
        await vector_db_client.close()
        

if __name__ == "__main__":
    # Ensure environment variables like OPENAI_API_KEY, RAG_EMBEDDING_MODEL are set if needed by EmbeddingGenerator
    # RAG_EMBEDDING_MODEL="sentence-transformers/all-MiniLM-L6-v2" # example
    # OPENAI_API_KEY="your_key_here" # example if using openai model

    # BEDROCK_SCRAPER_RATE_LIMIT=1.0 (Handled by scraper)
    # BEDROCK_DOCS_CACHE_TTL=86400 (Handled by scraper)

    # BACKEND_API_URL (Handled by VectorDBClient)

    asyncio.run(main())
